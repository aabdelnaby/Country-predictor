{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL PROJECT DELIVERABLE  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have decided to do some major changes to how I am going to approach this Query Machine.\n",
    "\n",
    "the dataset that I have created in the initial D2 contains global indexes of living indecators for alot of countries (about 150 countries), after rethinking i have taken two choices on how to deal with this dataset. These changes are:\n",
    "\n",
    "1- decrease the number of countries majorly (to ~50 countries), this is because most countries that exist within the same region have very similar metrics, this will make the classification task very hard and cause potential overfittiing for the data, which will even produce intersecting clusters. the countries will be chosen stratigically so that the dataset contains the hightest number of people (so i will include countries with high populations), also each region will have at least two countries (choosing qatar and KSA for example for the gulf ragion). This will ensure that the data prediction will have a higher accuracy.\n",
    "    \n",
    "2- use the data metrics from the chosen countires initial D2 dataset as random data generating points ( I will use each metric to be the median of a normal distribution), this will ensure to simulate a real life questionnaire data set according to the metrics that these data points were initially based on.  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- D2 FINAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-59f1f5c265ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.dpi']= 100\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn import cluster\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = pd.read_csv(\"data-source-grand1.csv\")\n",
    "df = pd.read_csv(\"data-freedom.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding freedom to the dg dataset\n",
    "for i in range(156):\n",
    "    if(dg[\"Country or region\"][i] in list(df['country'])):\n",
    "        n = df[df[\"country\"]==dg[\"Country or region\"][i]].index.values\n",
    "        dg[\"freedom\"][i] = df['freedom index'][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for nans\n",
    "np.sum(dg.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have decided to drop all nan data, this is because I am trying to minimize the countries included, dropping countries with nan fields will help with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = dg.dropna(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for choosing the countries, I am going to keep only the top 50 countries in terms of population, this will help decrease the error and build a (somewhat) accurate model.\n",
    "\n",
    "These countries are:\n",
    "China,\n",
    "India,\n",
    "United States,\n",
    "Indonesia,\n",
    "Pakistan,\n",
    "Brazil,\n",
    "Nigeria,\n",
    "Bangladesh,\n",
    "Russia,\n",
    "Mexico,\n",
    "Japan,\n",
    "Ethiopia,\n",
    "Philippines,\n",
    "Egypt,\n",
    "Vietnam,\n",
    "DR Congo,\n",
    "Turkey,\n",
    "Iran,\n",
    "Germany,\n",
    "Thailand,\n",
    "United Kingdom,\n",
    "France,\n",
    "Italy,\n",
    "Tanzania,\n",
    "South Africa,\n",
    "Myanmar,\n",
    "Kenya,\n",
    "South Korea,\n",
    "Colombia,\n",
    "Spain,\n",
    "Uganda,\n",
    "Argentina,\n",
    "Algeria,\n",
    "Sudan,\n",
    "Ukraine,\n",
    "Iraq,\n",
    "Afghanistan,\n",
    "Poland,\n",
    "Canada,\n",
    "Morocco,\n",
    "Saudi Arabia,\n",
    "Uzbekistan,\n",
    "Peru,\n",
    "Angola,\n",
    "Malaysia,\n",
    "Mozambique,\n",
    "Ghana,\n",
    "Yemen,\n",
    "Nepal.\n",
    "\n",
    "these countries represent 86.42% of the global world population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will drop all countries that are not in the list of the top 50 in population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset with top 50 countries in terms of population (Qatar is added because why not)\n",
    "top50 = pd.read_csv(\"top50.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top50_countries = top50[\"Country (or dependency)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top50_countries = top50_countries.drop(51) #cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top50_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the dataset to inlude the top 50 in terms of population\n",
    "#dg = dg.drop(6)\n",
    "for i in range(len(dg[\"Country or region\"])):\n",
    "    if(dg[\"Country or region\"][i] not in list(top50_countries)):\n",
    "        dg = dg.drop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that the dataset is finally complete, it is time to explore it and fine-tune it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the data set, the ranking field will be dropped because it does not serve a purpose, also the data needs to be rescaled on a scale 0-10, this scale is chosen because it is common and easier for people to classify based on such scale. random data that will be normally distributed will be generated based on the indices, these will be the best in simulating an in real life questionnaire, also will be helpfull in creating a K-NN classifier for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = dg.drop(\"rank\",axis =1)\n",
    "dg = dg.drop(\"Score\",axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = dg.drop(102) #i will remove nigeria because it overshadows other countries in the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring the data\n",
    "#GPD value :\n",
    "print(\"GPD per capita ranges from\",min(dg[\"GDP per capita\"]),\"to\",max(dg[\"GDP per capita\"]))\n",
    "#social support:\n",
    "print(\"social support ranges from\",min(dg[\"Social support\"]),\"to\",max(dg[\"Social support\"]))\n",
    "#Healthy life expectancy:\n",
    "print(\"Healthy life expectancy ranges from\",min(dg[\"Healthy life expectancy\"]),\"to\",max(dg[\"Healthy life expectancy\"]))\n",
    "#Freedom to make life choices\n",
    "print(\"Freedom to make life choices ranges from\",min(dg[\"Freedom to make life choices\"]),\"to\",max(dg[\"Freedom to make life choices\"]))\n",
    "#Generosity\n",
    "print(\"Generosity ranges from\",min(dg[\"Generosity\"]),\"to\",max(dg[\"Generosity\"]))\n",
    "#Perceptions of corruption\n",
    "print(\"Perceptions of corruption ranges from\",min(dg[\"Perceptions of corruption\"]),\"to\",max(dg[\"Perceptions of corruption\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = pd.DataFrame()\n",
    "gdict = {\"country\":[],\"GDP\":[],\"social support\":[],\"healthy life\":[],\"freedom of choice\":[],\"generosity\":[],\"corruption\":[]}\n",
    "for i in range(len(dg)):\n",
    "    for j in range(100):\n",
    "        gdict[\"country\"] += [list(dg[\"Country or region\"])[i]]\n",
    "        gdict[\"GDP\"] += [np.random.normal(list(dg[\"GDP per capita\"])[i],0.1,1)]\n",
    "        gdict[\"social support\"] += [np.random.normal(list(dg[\"Social support\"])[i],0.1,1)]\n",
    "        gdict[\"healthy life\"] += [np.random.normal(list(dg[\"Healthy life expectancy\"])[i],0.1,1)]\n",
    "        gdict[\"freedom of choice\"] += [np.random.normal(list(dg[\"Freedom to make life choices\"])[i],0.1,1)]\n",
    "        gdict[\"generosity\"] += [np.random.normal(list(dg[\"Generosity\"])[i],0.1,1)]\n",
    "        gdict[\"corruption\"] += [np.random.normal(list(dg[\"Perceptions of corruption\"])[i],0.1,1)]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = pd.DataFrame(gdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correcting the values format\n",
    "for i in range(len(gdf)):\n",
    "    for j in range(6):\n",
    "        gdf.loc[i][j+1] = gdf.loc[i][j+1][0] +1 #shifiting the data so the normal distribution wont give negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#creating historgrams usong seaborn\n",
    "import seaborn as sb\n",
    "\n",
    "sb.set(style=\"ticks\", color_codes=False)\n",
    "\n",
    "g = sb.pairplot(gdf, hue=\"country\", diag_kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- GDP seems like the factor with highest correlation with all other factors\n",
    "\n",
    "2- there seems to be a good correlation between social support and GDP, healthy life and freedom of choice.\n",
    "\n",
    "3- freedom of choice to be sparsely distributed making it hard to create correlaion from it, it might get dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#creating histogram using matplotlib\n",
    "fig, axs = plt.subplots(2,3)\n",
    "axs[0, 0].hist(gdf[\"GDP\"])\n",
    "axs[0, 0].set_title('GDP')\n",
    "axs[0, 1].hist(gdf[\"social support\"])\n",
    "axs[0, 1].set_title(\"social support\")\n",
    "axs[1, 0].hist(gdf[\"healthy life\"])\n",
    "axs[1, 0].set_title('healthy life')\n",
    "axs[1, 1].hist(gdf[\"freedom of choice\"])\n",
    "axs[1, 1].set_title('freedom of choice')\n",
    "axs[0, 2].hist(gdf[\"generosity\"])\n",
    "axs[0, 2].set_title('generosity')\n",
    "axs[1, 2].hist(gdf[\"corruption\"])\n",
    "axs[1, 2].set_title('corruption')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "box plots showing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"GPD boxplot\")\n",
    "plt.boxplot(gdf[\"GDP\"])\n",
    "plt.show()\n",
    "print(\"social supprt boxplot\")\n",
    "plt.boxplot(gdf[\"social support\"])\n",
    "plt.show()\n",
    "print(\"healthy life boxplot\")\n",
    "plt.boxplot(gdf[\"healthy life\"])\n",
    "plt.show()\n",
    "print(\"freedom of choice boxplot\")\n",
    "plt.boxplot(gdf[\"freedom of choice\"])\n",
    "plt.show()\n",
    "print(\"generosity boxplot\")\n",
    "plt.boxplot(gdf[\"generosity\"])\n",
    "plt.show()\n",
    "print(\"corruption boxplot\")\n",
    "plt.boxplot(gdf[\"corruption\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rescaling the features and preparing them for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = gdf['country']\n",
    "\n",
    "values = gdf.drop(['country'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#i will rescale all features to values in the range [0,1] (because it is easier for people to make a choice on that scale)\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "scaled_values =min_max_scaler.fit_transform(values)\n",
    "scaled_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying a K-NN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "counter = 0\n",
    "for i in [(1,'uniform'),(3,'distance'),(4,'uniform'),(10,'distance'),(20,'uniform'),(30,'distance'),(100,'uniform'),(150,'uniform'),(200,'uniform'),(7,'distance'),(66,'uniform'),(11,'distance'),(6,'uniform'),(43,'distance'),(10,'uniform'),(15,'uniform')]:\n",
    "    counter += 1\n",
    "    neighbors_num = i[0]\n",
    "    weights = i[1]\n",
    "    classifier = KNeighborsClassifier(n_neighbors=neighbors_num, weights=weights)\n",
    "    classifier.fit(scaled_values, targets)\n",
    "    prediction = classifier.predict(scaled_values)\n",
    "    mean_accuracy = np.mean(prediction == targets)\n",
    "    print(\"model number \",counter,\"- mean accuracy of model with \",i[0], \" neighbour(s) and\",i[1],\" weighting is:\",  mean_accuracy)\n",
    "    ####\n",
    "    cv_results = cross_validate(classifier, scaled_values, targets, cv=10, \n",
    "                            return_train_score=True)\n",
    "\n",
    "    print('Mean test score of model: {:.3f} (std: {:.3f})'\n",
    "          '\\nMean train score: {:.3f} (std: {:.3f})'.format(\n",
    "                                                  np.mean(cv_results['test_score']),\n",
    "                                                  np.std(cv_results['test_score']),\n",
    "                                                  np.mean(cv_results['train_score']),\n",
    "                                                  np.std(cv_results['train_score'])))\n",
    "    ####\n",
    "    \n",
    "   \n",
    "    print(\"box plot of cross-validation analysis:\")\n",
    "    plt.boxplot(cv_results['test_score'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics \n",
    "\n",
    "def learn_kNN_classifier(features_data, targets, neighbors, voting):\n",
    "    '''Set up a K-NN classifier and fits it to the given training data.\n",
    "       Return: learned classifier.''' \n",
    "    \n",
    "    classifier = KNeighborsClassifier(n_neighbors=neighbors, \n",
    "                                      weights=voting)\n",
    "    classifier.fit(features_data, targets)\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "\n",
    "def get_predictions(test_data, targets, classifier):\n",
    "    '''Input: trained classifier, test set for feature data paired with target values.\n",
    "       Return: prediction labels and the mean accuracy over the test set.'''\n",
    "    \n",
    "    Y_prediction = classifier.predict(test_data)\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(targets, Y_prediction)\n",
    "    \n",
    "    return (prediction, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " classi = learn_kNN_classifier(scaled_values, targets, 10, 'uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries =  dg['Country or region']\n",
    "countries =  list(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labeles_test =  train_test_split( values, targets,train_size= 0.8, shuffle=True)\n",
    "labels_train = np.array(labels_train).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 47\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "\n",
    "kmeans.fit(scaled_values)\n",
    "\n",
    "labels = np.array(countries)\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "centers_labels = kmeans.predict(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(centers_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.predict(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "y = gdf['country'].values\n",
    "transfomed_label = encoder.fit_transform(y)\n",
    "print(y)\n",
    "print(max(transfomed_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_data = gdf.drop(['country'], axis =1)\n",
    "x = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data))\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_values, transfomed_label,test_size = 0.35,random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "clf = MLPClassifier(random_state=1, max_iter=10000).fit(scaled_values, transfomed_label)\n",
    "mlp_prediction = clf.predict[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y[clf.predict([x_test[5]])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using the MLP as the final classifier as it is highly dimensional and it also provided the highest accuracy (64%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf=RandomForestClassifier(n_estimators=100,random_state=1)\n",
    "rf.fit(x_train,y_train)\n",
    "prediction_rf=rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.predict([[0.5,0.2,0.4,0.6,0.2,0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['figure.dpi']= 500\n",
    "rf_cm=confusion_matrix(y_test,prediction_rf)\n",
    "\n",
    "ax = sns.heatmap(rf_cm, square=True, annot=True, fmt='d', cbar=True,\n",
    "                     xticklabels=countries,\n",
    "                     yticklabels=countries, cmap=\"seismic\", annot_kws={\"size\":3})\n",
    "\n",
    "plt.ylim(47, 0)\n",
    "\n",
    "cbar = ax.collections[0].colorbar\n",
    "ax.tick_params(axis='both', which='major', labelsize=5)\n",
    "\n",
    "\n",
    "plt.xlabel('True label', fontsize=8)\n",
    "plt.ylabel('Predicted label', fontsize=8)\n",
    "plt.show()\n",
    "accuracy = metrics.accuracy_score(y_test, prediction_rf)\n",
    "print(\"the accuracy for this model is \", accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a 57% accuracy for such model is pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm=SVC(random_state=1,probability=True)\n",
    "svm.fit(x_train,y_train)\n",
    "svm_prediction=svm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.predict([x_test[0]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = svm.predict_proba([x_test[55]])\n",
    "h = np.array(h)\n",
    "h = np.transpose(h)\n",
    "max(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = svm.predict_proba(x_test)[0]\n",
    "\n",
    "# gets a dictionary of {'class_name': probability}\n",
    "prob_per_class_dictionary = dict(zip(svm.classes_, results))\n",
    "\n",
    "# gets a list of ['most_probable_class', 'second_most_probable_class', ..., 'least_class']\n",
    "results_ordered_by_probability = map(lambda x: x[0], sorted(zip(svm.classes_, results), key=lambda x: x[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "svm_cm=confusion_matrix(y_test,svm_prediction)\n",
    "\n",
    "\n",
    "ax = sns.heatmap(svm_cm, square=True, annot=True, fmt='d', cbar=True,\n",
    "                     xticklabels=countries,\n",
    "                     yticklabels=countries, cmap=\"seismic\", annot_kws={\"size\":3})\n",
    "\n",
    "plt.ylim(47, 0)\n",
    "\n",
    "cbar = ax.collections[0].colorbar\n",
    "ax.tick_params(axis='both', which='major', labelsize=5)\n",
    "\n",
    "plt.xlabel('True label', fontsize=8)\n",
    "plt.ylabel('Predicted label', fontsize=8)\n",
    "plt.show()\n",
    "accuracy = metrics.accuracy_score(y_test, svm_prediction)\n",
    "print(\"the accuracy for SVM is: \",accuracy*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors =13) # n_neighbors = k\n",
    "knn.fit(x_train,y_train)\n",
    "knn_prediction = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "cm_knn=confusion_matrix(y_test,knn_prediction)\n",
    "ax = sns.heatmap(cm_knn, square=True, annot=True, fmt='d', cbar=True,\n",
    "                     xticklabels=countries,\n",
    "                     yticklabels=countries, cmap=\"seismic\", annot_kws={\"size\":3})\n",
    "\n",
    "plt.ylim(47, 0)\n",
    "\n",
    "cbar = ax.collections[0].colorbar\n",
    "ax.tick_params(axis='both', which='major', labelsize=5)\n",
    "\n",
    "plt.xlabel('True label', fontsize=8)\n",
    "plt.ylabel('Predicted label', fontsize=8)\n",
    "plt.show()\n",
    "accuracy = metrics.accuracy_score(y_test, knn_prediction)\n",
    "print(\"the accuracy for this model is: \",accuracy*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import simpledialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class svmclassifier:\n",
    "    \n",
    "    #def __init__(self):\n",
    "        \n",
    "    \n",
    "    def mainscreen(self):    \n",
    "        self.window = tk.Tk()\n",
    "        self.corrpt = tk.IntVar()\n",
    "        self.GDP = tk.IntVar()\n",
    "        self.socialsupp = tk.IntVar()\n",
    "        self.foc = tk.IntVar()\n",
    "        self.genros = tk.IntVar()\n",
    "        self.health = tk.IntVar()\n",
    "        self.window.title(\"country pridictor\")\n",
    "        \n",
    "        \n",
    "        corruption = simpledialog.askinteger(\"input\", 'how corrupt do you feel you country is on a scale from 1 to 10',parent = self.window)/10\n",
    "        GDP = simpledialog.askinteger(\"input\", 'how wealthy do you think your country is on a scale from 1 to 10',parent = self.window)/10\n",
    "        sociols = simpledialog.askinteger(\"input\", \"how strong is social support in you country is on a scale from 1 to 10\",parent = self.window)/10\n",
    "        freedomoc = simpledialog.askinteger(\"input\", \"how free is choice in your country on a scale from 1 to 10\",parent = self.window)/10\n",
    "        generous = simpledialog.askinteger(\"input\", 'how generous are people from your country is on a scale from 1 to 10',parent = self.window)/10\n",
    "        helth = simpledialog.askinteger(\"input\", 'how healthy are people in your country is on a scale from 1 to 10',parent = self.window)/105\n",
    "        \n",
    "        prediction = clf.predict([[GDP,sociols,helth,freedomoc,generous,corruption]])      \n",
    "        \n",
    "        h = clf.predict_proba([[GDP,sociols,helth,freedomoc,generous,corruption]])\n",
    "        h = np.array(h)\n",
    "        h = np.transpose(h)\n",
    "        h1 = max(h)\n",
    "        \n",
    "        #predictiontxt = tk.Text(self.window)\n",
    "        #predictiontxt.insert(INSERT,\"with {prob} confidence you are from {country}\".format(prob =h, country = prediction[0] ))\n",
    "        \n",
    "        print(\"with {prob}% confidence you are from {country}\".format(prob =100*h1[0], country = y[prediction[0]] ))\n",
    "\n",
    "        \n",
    "        self.window.destroy()\n",
    "\n",
    "        self.window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w = svmclassifier()\n",
    "w.mainscreen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# discussion and limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is clear that there are many limitations to this classifier, first and most important is that it relies on the subjective opinions of people that see things within a local scale while the data is put based on a global scale, also the method of the generation of the data is a bit questionable as it does not rely directly on survies but rather on indices, it is hard for an inde alone to decide alot of varying factors for a country, espicially that the index does not come with a corresponding variance with it, missing around with the variance of the data and choosong a more well educated value would change the results of the classifier to be drastically better in real life testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
